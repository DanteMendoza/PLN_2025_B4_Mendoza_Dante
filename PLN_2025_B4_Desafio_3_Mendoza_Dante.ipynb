{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del lenguaje natural - 2025 - B4 - Desafio 3\n",
        "**Inteligencia Artificial - CEIA - FIUBA**\n",
        "\n",
        "## Autor\n",
        "\n",
        "- **Mendoza Dante**.\n",
        "- **SIU: e2206**.\n",
        "\n",
        "**Nota:** Tomé como base el código compartido por los docentes. Como la idea era pasar de Keras a PyTorch, utilicé IA para consultar errores y adaptación de código. Las que utilicé fueron Copilot y ChatGPT. Elegí un ebook encontrado en el sitio: https://www.textos.info/alejandro-dumas/el-conde-de-montecristo/ebook"
      ],
      "metadata": {
        "id": "B_R4c51a0zGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "## Modelo de lenguaje con tokenización por caracteres\n",
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusiones\n",
        "\n",
        "ALGO..."
      ],
      "metadata": {
        "id": "wsw0OLXaQxOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Bibliotecas y entorno CPU o GPU\n",
        "################################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gradio as gr\n",
        "\n",
        "# CPU o GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSpUUlOrsbti",
        "outputId": "701f5ed2-2215-4522-a99c-a23845a8279a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Variables globales\n",
        "# ==============================\n",
        "EPOCHS = 10\n",
        "PATIENCE = 5\n",
        "MAX_LEN = 10 # Caracteres\n",
        "TEMPERATURE = 0.8"
      ],
      "metadata": {
        "id": "Bu4HmUoOHrnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Descarga del texto\n",
        "################################################################################\n",
        "url = \"https://www.textos.info/alejandro-dumas/el-conde-de-montecristo/ebook\"\n",
        "html = requests.get(url).content\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "paragraphs = soup.find_all(\"p\")\n",
        "text = \" \".join(p.text for p in paragraphs)\n",
        "\n",
        "# limpieza básica: minúsculas y caracteres permitidos\n",
        "text = text.lower()\n",
        "text = re.sub(r\"[^a-záéíóúüñ0-9,.!?;:()\\s-]\", \" \", text)\n",
        "\n",
        "print(\"Longitud del corpus:\", len(text))\n",
        "print(\"Ejemplo:\", text[:500])"
      ],
      "metadata": {
        "id": "mUqy1c0VEvHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a18c015-83eb-4884-95a8-8a6be8438c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del corpus: 2573547\n",
            "Ejemplo:  el 24 de febrero de 1815, el vigía de nuestra señora de la guarda dio\r\n",
            " la señal de que se hallaba a la vista el bergantín el faraón procedente\r\n",
            " de esmirna, trieste y nápoles. como suele hacerse en tales casos, salió\r\n",
            " inmediatamente en su busca un práctico, que pasó por delante del \r\n",
            "castillo de if y subió a bordo del buque entre la isla de rión y el cabo\r\n",
            " mongión. en un instante, y también como de costum bre, se llenó de \r\n",
            "curiosos la plataforma del castillo de san juan, por que en marsella\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Tokenización\n",
        "################################################################################\n",
        "chars_vocab = sorted(list(set(text)))\n",
        "vocab_size = len(chars_vocab)\n",
        "\n",
        "char2idx = {c:i for i,c in enumerate(chars_vocab)}\n",
        "idx2char = {i:c for c,i in char2idx.items()}\n",
        "\n",
        "# tokenizo\n",
        "tokenized_text = [char2idx[c] for c in text]"
      ],
      "metadata": {
        "id": "OPDJhCV4QMlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Preparo el dataset\n",
        "################################################################################\n",
        "max_context_size = 100\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))\n",
        "\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]\n",
        "\n",
        "train_seqs = [train_text[i:i+max_context_size] for i in range(len(train_text)-max_context_size)]\n",
        "val_seqs = [val_text[i*max_context_size:(i+1)*max_context_size] for i in range(num_val)]\n",
        "\n",
        "X_train = np.array(train_seqs[:-1])\n",
        "y_train = np.array(train_seqs[1:])\n",
        "\n",
        "X_val = np.array(val_seqs[:-1])\n",
        "y_val = np.array(val_seqs[1:])"
      ],
      "metadata": {
        "id": "FKWSmSNGQ3Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Adapto a PyTorch\n",
        "################################################################################\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(CharDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(CharDataset(X_val, y_val), batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Ji9mZ0AAQ95X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Modelo RNN con Embeddings\n",
        "################################################################################\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, model_type, vocab_size, emb_size=64, hidden_size=200, n_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
        "        self.model_type = model_type.lower()\n",
        "        if self.model_type == \"simplernn\":\n",
        "            self.rnn = nn.RNN(emb_size, hidden_size, num_layers=n_layers, batch_first=True, nonlinearity='tanh', dropout=dropout if n_layers>1 else 0)\n",
        "        elif self.model_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(emb_size, hidden_size, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers>1 else 0)\n",
        "        elif self.model_type == \"gru\":\n",
        "            self.rnn = nn.GRU(emb_size, hidden_size, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers>1 else 0)\n",
        "        else:\n",
        "            raise ValueError(\"model_type debe ser simplernn|lstm|gru\")\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.emb(x)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        num_layers = self.rnn.num_layers\n",
        "        hidden_size = self.rnn.hidden_size\n",
        "        if self.model_type == \"lstm\":\n",
        "            h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
        "            c0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
        "            return (h0, c0)\n",
        "        else:\n",
        "            h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
        "            return h0"
      ],
      "metadata": {
        "id": "6aRVNJ5cRFJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Loop de entrenamiento con perplejidad\n",
        "################################################################################\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, _ = model(x)\n",
        "            B,T,V = logits.size()\n",
        "            loss = criterion(logits.view(B*T,V), y.view(B*T))\n",
        "            total_loss += loss.item()*(B*T)\n",
        "            total_tokens += B*T\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss, math.exp(avg_loss)\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=EPOCHS, lr=1e-3, clip=5.0, patience=PATIENCE):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_ppl = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(x)\n",
        "            B,T,V = logits.size()\n",
        "            loss = criterion(logits.view(B*T,V), y.view(B*T))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()*(B*T)\n",
        "            total_tokens += B*T\n",
        "        train_loss = total_loss / total_tokens\n",
        "\n",
        "        val_loss, val_ppl = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_ppl {val_ppl:.4f}\")\n",
        "\n",
        "        if val_ppl < best_ppl:\n",
        "            best_ppl = val_ppl\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping por perplejidad\")\n",
        "                break\n",
        "    model.load_state_dict(best_state)\n",
        "    return model"
      ],
      "metadata": {
        "id": "w1sl7VnVRL4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Funciones de generación\n",
        "################################################################################\n",
        "def greedy_generate(model, seed, stoi, itos, max_len=MAX_LEN):\n",
        "    model.eval()\n",
        "    idxs = [stoi[c] for c in seed if c in stoi]\n",
        "    input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n",
        "    hidden = None\n",
        "    generated = idxs.copy()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            logits, hidden = model(input_seq, hidden)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            topi = torch.argmax(probs, dim=-1).item()\n",
        "            generated.append(topi)\n",
        "            input_seq = torch.tensor([[topi]], device=device)\n",
        "    return \"\".join([itos[i] for i in generated])\n",
        "\n",
        "def stochastic_generate(model, seed, stoi, itos, max_len=MAX_LEN, temperature=TEMPERATURE):\n",
        "    model.eval()\n",
        "    idxs = [stoi[c] for c in seed if c in stoi]\n",
        "    input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n",
        "    hidden = None\n",
        "    generated = idxs.copy()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            logits, hidden = model(input_seq, hidden)\n",
        "            probs = F.softmax(logits[:, -1, :].squeeze(0)/temperature, dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated.append(next_idx)\n",
        "            input_seq = torch.tensor([[next_idx]], device=device)\n",
        "    return \"\".join([itos[i] for i in generated])"
      ],
      "metadata": {
        "id": "aHMyjaP-RUEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Interfaz Gradio\n",
        "################################################################################\n",
        "def model_response(ingresar_texto):\n",
        "    output_text = greedy_generate(model, ingresar_texto, char2idx, idx2char, max_len=MAX_LEN)\n",
        "    return output_text\n",
        "\n",
        "iface = gr.Interface(fn=model_response, inputs=\"text\", outputs=\"text\")"
      ],
      "metadata": {
        "id": "iC9XAXnevHgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Entreno el modelo\n",
        "################################################################################\n",
        "model_type = \"lstm\"  # simplernn | lstm | gru\n",
        "model = CharRNN(model_type, vocab_size=vocab_size, emb_size=64, hidden_size=200, n_layers=2, dropout=0.1)\n",
        "model = train(model, train_loader, val_loader, epochs=EPOCHS, lr=1e-3, clip=5.0, patience=PATIENCE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvMD-veDSXKi",
        "outputId": "ff2e969d-85c4-4d47-f327-8cc5c10667bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 9048/9048 [06:06<00:00, 24.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | train_loss 1.2482 | val_loss 7.9340 | val_ppl 2790.4362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 9048/9048 [06:07<00:00, 24.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | train_loss 1.0852 | val_loss 8.3614 | val_ppl 4278.7209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 9048/9048 [06:07<00:00, 24.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | train_loss 1.0575 | val_loss 8.5659 | val_ppl 5249.3160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 9048/9048 [06:07<00:00, 24.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | train_loss 1.0448 | val_loss 8.6300 | val_ppl 5596.9380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 9048/9048 [06:07<00:00, 24.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | train_loss 1.0374 | val_loss 8.8186 | val_ppl 6758.8649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 9048/9048 [06:07<00:00, 24.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | train_loss 1.0326 | val_loss 8.9481 | val_ppl 7693.2326\n",
            "Early stopping por perplejidad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Guardado del modelo\n",
        "################################################################################\n",
        "torch.save(model, \"char_rnn_full.pth\") # se que se guarda en la memoria de la sesion pero sirve para descargarlo"
      ],
      "metadata": {
        "id": "a7MmDGVKY0tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Carga del modelo\n",
        "################################################################################\n",
        "model = torch.load(\"char_rnn_full.pth\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "7Xb9C2Xr2U0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Lanzo Gradio\n",
        "################################################################################\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "o-tFODnB1eZm",
        "outputId": "c4215ded-e5b0-4bf0-8598-c4310e58c205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7d37daa90df79c8368.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7d37daa90df79c8368.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7d37daa90df79c8368.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Beam Search (determinístico y estocástico)\n",
        "################################################################################\n",
        "from scipy.special import softmax\n",
        "\n",
        "def beam_search(model, seed, stoi, itos, max_len=MAX_LEN, beam_width=3, temperature=TEMPERATURE, stochastic=False):\n",
        "    model.eval()\n",
        "    idxs = [stoi[c] for c in seed if c in stoi]\n",
        "    sequences = [(idxs, 0.0, None)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # inicializo hidden con todo el contexto\n",
        "        input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n",
        "        _, hidden = model(input_seq, None)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            all_candidates = []\n",
        "            for seq, log_prob, hid in sequences:\n",
        "                last_token = torch.tensor([[seq[-1]]], device=device)\n",
        "                logits, new_hid = model(last_token, hidden if hid is None else hid)\n",
        "                probs = F.softmax(logits[:, -1, :].squeeze(0), dim=-1).cpu().numpy()\n",
        "\n",
        "                if stochastic:\n",
        "                    # muestreo estocástico con temperatura\n",
        "                    probs_temp = softmax(np.log(probs+1e-20)/temperature)\n",
        "                    top_idxs = np.random.choice(len(probs_temp), beam_width, p=probs_temp)\n",
        "                else:\n",
        "                    # determinístico: top k\n",
        "                    top_idxs = probs.argsort()[-beam_width:][::-1]\n",
        "\n",
        "                for idx in top_idxs:\n",
        "                    candidate = (seq + [idx], log_prob + math.log(probs[idx]+1e-20), new_hid)\n",
        "                    all_candidates.append(candidate)\n",
        "\n",
        "            # me quedo con los beam_width mejores\n",
        "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # devuelvo la secuencia con mayor log_prob\n",
        "        best_seq = sequences[0][0]\n",
        "        return \"\".join([itos[i] for i in best_seq])"
      ],
      "metadata": {
        "id": "16yePuHAcZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Utilizo Beam Search (determinístico y estocástico)\n",
        "################################################################################\n",
        "seed_text = \"el cond\"\n",
        "print(\"Greedy:\", greedy_generate(model, seed_text, char2idx, idx2char, max_len=MAX_LEN))\n",
        "print(\"Beam search det:\", beam_search(model, seed_text, char2idx, idx2char, max_len=MAX_LEN, beam_width=5, stochastic=False))\n",
        "print(f\"Beam search sto (T={TEMPERATURE}):\", beam_search(model, seed_text, char2idx, idx2char, max_len=MAX_LEN, beam_width=5, stochastic=True, temperature=TEMPERATURE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnRTuNt7cbNV",
        "outputId": "99f55c14-6be9-4b31-b841-c8ae9a5ee408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: el conde de monte\n",
            "Beam search det: el conde de morce\n",
            "Beam search sto (T=0.8): el condo de monte\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del lenguaje natural - 2025 - B4 - Desafio 4\n",
        "**Inteligencia Artificial - CEIA - FIUBA**\n",
        "\n",
        "## Autor\n",
        "\n",
        "- **Mendoza Dante**.\n",
        "- **SIU: e2206**.\n",
        "\n",
        "**Nota:** Tomé como base el código compartido por los docentes. Como la idea era pasar de Keras a PyTorch, utilicé IA para consultar errores y adaptación de código. Las que utilicé fueron Copilot y ChatGPT."
      ],
      "metadata": {
        "id": "B_R4c51a0zGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "## LSTM Bot QA\n",
        "### Consigna\n",
        "El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\n",
        "\n",
        "Link: http://convai.io/data/\n",
        "\n",
        "### Recomendaciones:\n",
        "- MAX_VOCAB_SIZE = 8000\n",
        "- max_length ~ 10\n",
        "- Embeddings 300 Fasttext\n",
        "- n_units = 128\n",
        "- LSTM Dropout 0.2\n",
        "- Epochs 30~50\n",
        "\n",
        "### Preguntas interesantes:\n",
        "- Do you read?\n",
        "- Do you have any pet?\n",
        "- Where are you from?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Leo los datos y hago una pequeña normalizacion\n",
        "# ================================================================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "\n",
        "# Parámetros\n",
        "DATA_FILENAME = \"data_volunteers.json\"\n",
        "DRIVE_ID = \"1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN\"\n",
        "MAX_LENGTH = 10   # tokens por frase\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "# Descargo si no existe\n",
        "if not Path(DATA_FILENAME).exists():\n",
        "    print(\"Descargando dataset...\")\n",
        "    url = f\"https://drive.google.com/uc?id={DRIVE_ID}&export=download\"\n",
        "    gdown.download(url, DATA_FILENAME, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado:\", DATA_FILENAME)\n",
        "\n",
        "# Cargo el JSON\n",
        "with open(DATA_FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Registros en JSON:\", len(data))\n",
        "print(\"Ejemplo keys de la primera entrada:\", list(data[0].keys()))\n",
        "\n",
        "# Limpieza / normalización\n",
        "def clean_text(txt):\n",
        "    if not isinstance(txt, str):\n",
        "        return \"\"\n",
        "    txt = txt.lower().strip()\n",
        "    # reemplazos simples de contracciones\n",
        "    txt = txt.replace(\"i'm\", \"i am\")\n",
        "    txt = txt.replace(\"you're\", \"you are\")\n",
        "    txt = txt.replace(\"he's\", \"he is\")\n",
        "    txt = txt.replace(\"she's\", \"she is\")\n",
        "    txt = txt.replace(\"it's\", \"it is\")\n",
        "    txt = txt.replace(\"that's\", \"that is\")\n",
        "    txt = txt.replace(\"what's\", \"what is\")\n",
        "    txt = txt.replace(\"where's\", \"where is\")\n",
        "    txt = txt.replace(\"don't\", \"do not\")\n",
        "    txt = txt.replace(\"doesn't\", \"does not\")\n",
        "    txt = txt.replace(\"didn't\", \"did not\")\n",
        "    txt = txt.replace(\"won't\", \"will not\")\n",
        "    txt = txt.replace(\"can't\", \"can not\")\n",
        "    txt = txt.replace(\"'ll\", \" will\")\n",
        "    txt = txt.replace(\"'ve\", \" have\")\n",
        "    txt = txt.replace(\"'re\", \" are\")\n",
        "    txt = txt.replace(\"'d\", \" would\")\n",
        "    # para eliminar caracteres no alfanuméricos excepto espacios\n",
        "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)\n",
        "    # colapsar espacios múltiples\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "# Extraigo pares input-output\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []  # decoder inputs with <sos>\n",
        "max_len_in_chars = 0\n",
        "\n",
        "for entry in data:\n",
        "    dialog = entry.get(\"dialog\", [])\n",
        "    # aseguramos al menos 2 turns\n",
        "    if not dialog or len(dialog) < 2:\n",
        "        continue\n",
        "    for i in range(len(dialog) - 1):\n",
        "        a = clean_text(dialog[i].get(\"text\", \"\"))\n",
        "        b = clean_text(dialog[i+1].get(\"text\", \"\"))\n",
        "        if not a or not b:\n",
        "            continue\n",
        "        # token-length check (por tokens)\n",
        "        tokens_a = a.split()\n",
        "        tokens_b = b.split()\n",
        "        if len(tokens_a) > MAX_LENGTH or len(tokens_b) > MAX_LENGTH:\n",
        "            continue\n",
        "        # construyo las cadenas para encoder/decoder\n",
        "        input_sentence = a\n",
        "        output_sentence = b + \" <eos>\" # decoder target ends with <eos>\n",
        "        output_sentence_input = \"<sos> \" + b # decoder input starts with <sos>\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "        max_len_in_chars = max(max_len_in_chars, len(a), len(b))\n",
        "\n",
        "print(\"Cantidad de pares (después de filtrar por MAX_LENGTH={} tokens): {}\".format(MAX_LENGTH, len(input_sentences)))\n",
        "print(\"Longest example length (chars):\", max_len_in_chars)\n",
        "\n",
        "# Mostrar algunos ejemplos aleatorios\n",
        "print(\"\\nAlgunos ejemplos de pares (input -> output):\\n\")\n",
        "n_show = 8\n",
        "idxs = random.sample(range(len(input_sentences)), min(n_show, len(input_sentences)))\n",
        "for i in idxs:\n",
        "    print(\"IN : \", input_sentences[i])\n",
        "    print(\"OUT: \", output_sentences_inputs[i], \" -> target:\", output_sentences[i])\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Guardamos\n",
        "out_obj = {\n",
        "    \"input_sentences\": input_sentences,\n",
        "    \"output_sentences_inputs\": output_sentences_inputs,\n",
        "    \"output_sentences\": output_sentences\n",
        "}\n",
        "with open(\"pairs_preprocessed.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(out_obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSe guardó pairs_preprocessed.json con los pares procesados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6_ZqJj3hYVr",
        "outputId": "8cca117f-5de6-46c2-aaa0-c8f7af617e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado: data_volunteers.json\n",
            "Registros en JSON: 1111\n",
            "Ejemplo keys de la primera entrada: ['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id']\n",
            "Cantidad de pares (después de filtrar por MAX_LENGTH=10 tokens): 9886\n",
            "Longest example length (chars): 87\n",
            "\n",
            "Algunos ejemplos de pares (input -> output):\n",
            "\n",
            "IN :  no\n",
            "OUT:  <sos> oh okay you have any pets  -> target: oh okay you have any pets <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  which sports\n",
            "OUT:  <sos> i work in a computer company i love american sports  -> target: i work in a computer company i love american sports <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  what do you do for work\n",
            "OUT:  <sos> struggle  -> target: struggle <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  fuking\n",
            "OUT:  <sos> i love to read  -> target: i love to read <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  my name is lucy what is yours\n",
            "OUT:  <sos> my name is ranjan  -> target: my name is ranjan <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  it is ok what are you up to\n",
            "OUT:  <sos> i just got back from the gym  -> target: i just got back from the gym <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  i have none but we will go that step soon\n",
            "OUT:  <sos> i have three children do you have any hobbies  -> target: i have three children do you have any hobbies <eos>\n",
            "------------------------------------------------------------\n",
            "IN :  hahn do you work\n",
            "OUT:  <sos> yes i work at home  -> target: yes i work at home <eos>\n",
            "------------------------------------------------------------\n",
            "\n",
            "Se guardó pairs_preprocessed.json con los pares procesados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Tokenización y creación de vocabularios\n",
        "# ================================================================================\n",
        "import json\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Parámetros\n",
        "MAX_VOCAB_SIZE = 8000\n",
        "\n",
        "# Cargo los pares procesados\n",
        "with open(\"pairs_preprocessed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    pairs = json.load(f)\n",
        "\n",
        "input_sentences = pairs[\"input_sentences\"]\n",
        "output_sentences_inputs = pairs[\"output_sentences_inputs\"]\n",
        "output_sentences = pairs[\"output_sentences\"]\n",
        "\n",
        "print(\"Total de pares cargados:\", len(input_sentences))\n",
        "\n",
        "# ================================================================================\n",
        "# Tokenizar\n",
        "# ================================================================================\n",
        "input_tokens = [s.split() for s in input_sentences]\n",
        "output_tokens_in = [s.split() for s in output_sentences_inputs]\n",
        "output_tokens_out = [s.split() for s in output_sentences]\n",
        "\n",
        "# ================================================================================\n",
        "# Crear vocabularios\n",
        "# ================================================================================\n",
        "def build_vocab(token_lists, max_vocab_size, add_specials=True):\n",
        "    freq = Counter([tok for sent in token_lists for tok in sent])\n",
        "    most_common = freq.most_common(max_vocab_size - 4 if add_specials else max_vocab_size)\n",
        "\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    specials = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"] if add_specials else []\n",
        "\n",
        "    for idx, word in enumerate(specials + [w for w, _ in most_common]):\n",
        "        word2idx[word] = idx\n",
        "        idx2word[idx] = word\n",
        "\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx_inputs, idx2word_inputs = build_vocab(input_tokens, MAX_VOCAB_SIZE)\n",
        "word2idx_outputs, idx2word_outputs = build_vocab(output_tokens_out, MAX_VOCAB_SIZE)\n",
        "\n",
        "num_words_input = len(word2idx_inputs)\n",
        "num_words_output = len(word2idx_outputs)\n",
        "\n",
        "print(f\"Vocabulario INPUT: {num_words_input} palabras\")\n",
        "print(f\"Vocabulario OUTPUT: {num_words_output} palabras\")\n",
        "\n",
        "# ================================================================================\n",
        "# Convertir tokens -> índices (padding)\n",
        "# ================================================================================\n",
        "def encode_sentences(token_lists, word2idx, max_len=None):\n",
        "    sequences = []\n",
        "    if not max_len:\n",
        "        max_len = max(len(s) for s in token_lists)\n",
        "    for sent in token_lists:\n",
        "        seq = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in sent]\n",
        "        # padding\n",
        "        if len(seq) < max_len:\n",
        "            seq += [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
        "        else:\n",
        "            seq = seq[:max_len]\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences), max_len\n",
        "\n",
        "encoder_input_sequences, max_input_len = encode_sentences(input_tokens, word2idx_inputs)\n",
        "decoder_input_sequences, max_out_len_in = encode_sentences(output_tokens_in, word2idx_outputs)\n",
        "decoder_target_sequences, max_out_len_out = encode_sentences(output_tokens_out, word2idx_outputs)\n",
        "\n",
        "# La longitud máxima de salida se toma igual para decoder input y target\n",
        "max_out_len = max(max_out_len_in, max_out_len_out)\n",
        "\n",
        "print(f\"max_input_len = {max_input_len}\")\n",
        "print(f\"max_out_len = {max_out_len}\")\n",
        "\n",
        "# ================================================================================\n",
        "# Convertir a tensores PyTorch\n",
        "# ================================================================================\n",
        "encoder_input_sequences = torch.tensor(encoder_input_sequences, dtype=torch.long)\n",
        "decoder_input_sequences = torch.tensor(decoder_input_sequences, dtype=torch.long)\n",
        "decoder_target_sequences = torch.tensor(decoder_target_sequences, dtype=torch.long)\n",
        "\n",
        "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
        "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
        "print(\"decoder_target_sequences.shape:\", decoder_target_sequences.shape)\n",
        "\n",
        "# ================================================================================\n",
        "# Muestro ejemplo decodificado\n",
        "# ================================================================================\n",
        "def decode_sequence(seq, idx2word):\n",
        "    words = [idx2word.get(idx, \"<unk>\") for idx in seq if idx != word2idx_inputs[\"<pad>\"]]\n",
        "    return \" \".join(words)\n",
        "\n",
        "example_idx = np.random.randint(0, len(encoder_input_sequences))\n",
        "print(\"\\nEjemplo de secuencia codificada y decodificada:\")\n",
        "print(\"Encoder input:\", decode_sequence(encoder_input_sequences[example_idx].tolist(), idx2word_inputs))\n",
        "print(\"Decoder input:\", decode_sequence(decoder_input_sequences[example_idx].tolist(), idx2word_outputs))\n",
        "print(\"Target:\", decode_sequence(decoder_target_sequences[example_idx].tolist(), idx2word_outputs))"
      ],
      "metadata": {
        "id": "mUqy1c0VEvHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78463e54-a8bf-4526-dedb-0511e2c21a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de pares cargados: 9886\n",
            "Vocabulario INPUT: 2932 palabras\n",
            "Vocabulario OUTPUT: 2935 palabras\n",
            "max_input_len = 10\n",
            "max_out_len = 11\n",
            "encoder_input_sequences.shape: torch.Size([9886, 10])\n",
            "decoder_input_sequences.shape: torch.Size([9886, 11])\n",
            "decoder_target_sequences.shape: torch.Size([9886, 11])\n",
            "\n",
            "Ejemplo de secuencia codificada y decodificada:\n",
            "Encoder input: what kind of dog is he\n",
            "Decoder input: <sos> german shepherd\n",
            "Target: german shepherd <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Preparo embeddings GloVe 300d\n",
        "# ================================================================================\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Descargar GloVe si no existe\n",
        "glove_zip = \"glove.6B.zip\"\n",
        "glove_dir = \"glove.6B\"\n",
        "if not os.path.exists(glove_zip):\n",
        "    print(\"Descargando GloVe (puede tardar unos minutos)...\")\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "else:\n",
        "    print(\"GloVe ya está descargado.\")\n",
        "\n",
        "# Parámetros\n",
        "EMBEDDING_DIM = 300\n",
        "glove_path = f\"glove.6B.{EMBEDDING_DIM}d.txt\"\n",
        "\n",
        "# Cargar embeddings a un diccionario\n",
        "print(\"Cargando vectores GloVe en memoria...\")\n",
        "embeddings_index = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "print(f\"Total de vectores GloVe cargados: {len(embeddings_index):,}\")\n",
        "\n",
        "# ================================================================================\n",
        "# Construir matriz de embeddings para vocabulario de INPUT\n",
        "# ================================================================================\n",
        "embedding_matrix_inputs = np.zeros((len(word2idx_inputs), EMBEDDING_DIM))\n",
        "not_found_in_glove = 0\n",
        "\n",
        "for word, idx in word2idx_inputs.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix_inputs[idx] = embeddings_index[word]\n",
        "    else:\n",
        "        not_found_in_glove += 1\n",
        "        embedding_matrix_inputs[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
        "\n",
        "print(f\"Palabras del vocabulario INPUT no encontradas en GloVe: {not_found_in_glove}\")\n",
        "\n",
        "# Convertir a tensor PyTorch\n",
        "embedding_matrix_inputs = torch.tensor(embedding_matrix_inputs, dtype=torch.float32)\n",
        "\n",
        "# ================================================================================\n",
        "# Crear capa de embeddings\n",
        "# ================================================================================\n",
        "embedding_inputs = torch.nn.Embedding.from_pretrained(embedding_matrix_inputs, freeze=False)\n",
        "print(\"Capa de embeddings (INPUT) creada:\", embedding_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7vjgB0AUkN0",
        "outputId": "4dfe9326-0013-46f9-d376-fc58f760b452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando GloVe (puede tardar unos minutos)...\n",
            "--2025-10-12 16:15:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-10-12 16:15:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-10-12 16:15:42--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-10-12 16:18:22 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Cargando vectores GloVe en memoria...\n",
            "Total de vectores GloVe cargados: 400,000\n",
            "Palabras del vocabulario INPUT no encontradas en GloVe: 256\n",
            "Capa de embeddings (INPUT) creada: Embedding(2932, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Corrección de índices fuera de rango en los targets\n",
        "# ================================================================================\n",
        "# Reviso límites del vocabulario de salida\n",
        "vocab_size_out = len(word2idx_outputs)\n",
        "print(\"Tamaño del vocabulario de salida:\", vocab_size_out)\n",
        "print(\"Índices reservados:\")\n",
        "print(f\"<pad>: {word2idx_outputs['<pad>']}, <sos>: {word2idx_outputs['<sos>']}, <eos>: {word2idx_outputs['<eos>']}, <unk>: {word2idx_outputs['<unk>']}\")\n",
        "\n",
        "# Forzamos que todos los targets estén dentro del rango válido\n",
        "decoder_target_sequences = torch.clamp(decoder_target_sequences, max=vocab_size_out - 1)\n",
        "\n",
        "# También el decoder_input_sequences\n",
        "decoder_input_sequences = torch.clamp(decoder_input_sequences, max=vocab_size_out - 1)\n",
        "\n",
        "print(\"Corrección aplicada: todos los índices están dentro del rango válido [0, vocab_size-1].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbxnwK3mXjYP",
        "outputId": "37ff5b5c-318b-4143-b600-dbb62b9423c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario de salida: 2935\n",
            "Índices reservados:\n",
            "<pad>: 0, <sos>: 2, <eos>: 4, <unk>: 1\n",
            "Corrección aplicada: todos los índices están dentro del rango válido [0, vocab_size-1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Modelo Seq2Seq (LSTM) + Entrenamiento\n",
        "# ================================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)\n",
        "\n",
        "# ================================================================================\n",
        "# Creo Dataset y DataLoader\n",
        "# ================================================================================\n",
        "BATCH_SIZE = 64 # 64 me funciono bien\n",
        "\n",
        "dataset = TensorDataset(encoder_input_sequences, decoder_input_sequences, decoder_target_sequences)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "pad_idx = word2idx_outputs[\"<pad>\"]\n",
        "sos_idx = word2idx_outputs[\"<sos>\"]\n",
        "eos_idx = word2idx_outputs[\"<eos>\"]\n",
        "\n",
        "# ================================================================================\n",
        "# Defino Encoder y Decoder\n",
        "# ================================================================================\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim=128, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding.embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [batch]\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.6):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]  # primer token <sos>\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# ================================================================================\n",
        "# Instancio modelo, pérdida y optimizador\n",
        "# ================================================================================\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.2\n",
        "EPOCHS = 50 # 30 / 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "encoder = Encoder(embedding_inputs, hidden_dim=HIDDEN_DIM, dropout=DROPOUT)\n",
        "decoder = Decoder(vocab_size=len(word2idx_outputs), embedding_dim=300, hidden_dim=HIDDEN_DIM, dropout=DROPOUT)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ================================================================================\n",
        "# Entrenamiento\n",
        "# ================================================================================\n",
        "print(\"Comenzando entrenamiento...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for src, trg_in, trg_out in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        src, trg_in, trg_out = src.to(device), trg_in.to(device), trg_out.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg_in)\n",
        "        # output: [batch, trg_len, vocab_size]\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output_dim)   # ignoramos el primer <sos>\n",
        "        trg_out = trg_out[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg_out)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Pérdida promedio: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Entrenamiento finalizado\")\n",
        "\n",
        "# Guardar el modelo\n",
        "torch.save(model.state_dict(), \"seq2seq_glove_lstm.pt\")\n",
        "print(\"Modelo guardado como seq2seq_glove_lstm.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqu8DdYfWq0p",
        "outputId": "f97a80ad-6c05-48cf-9a60-bb94c5fb9f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comenzando entrenamiento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 155/155 [00:39<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Pérdida promedio: 4.8476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 155/155 [00:36<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50 - Pérdida promedio: 4.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 155/155 [00:35<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50 - Pérdida promedio: 4.1195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 155/155 [00:32<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50 - Pérdida promedio: 3.9719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 155/155 [00:33<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50 - Pérdida promedio: 3.8805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 155/155 [00:32<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 - Pérdida promedio: 3.8107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 155/155 [00:32<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50 - Pérdida promedio: 3.7068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50 - Pérdida promedio: 3.6318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50 - Pérdida promedio: 3.5405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 155/155 [00:36<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 - Pérdida promedio: 3.4887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 155/155 [00:32<00:00,  4.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 - Pérdida promedio: 3.4269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 155/155 [00:32<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 - Pérdida promedio: 3.3470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 155/155 [00:32<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 - Pérdida promedio: 3.2957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 155/155 [00:32<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 - Pérdida promedio: 3.2076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 155/155 [00:31<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 - Pérdida promedio: 3.1431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 155/155 [00:31<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 - Pérdida promedio: 3.1041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 155/155 [00:32<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 - Pérdida promedio: 3.0708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 155/155 [00:32<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 - Pérdida promedio: 2.9859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 155/155 [00:32<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 - Pérdida promedio: 2.9119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 155/155 [00:32<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 - Pérdida promedio: 2.8720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 155/155 [00:32<00:00,  4.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 - Pérdida promedio: 2.8392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 155/155 [00:32<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 - Pérdida promedio: 2.7562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 155/155 [00:32<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 - Pérdida promedio: 2.7016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 155/155 [00:32<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 - Pérdida promedio: 2.6618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 155/155 [00:32<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 - Pérdida promedio: 2.6327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 155/155 [00:35<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 - Pérdida promedio: 2.5814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 155/155 [00:33<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 - Pérdida promedio: 2.5187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 - Pérdida promedio: 2.4693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 155/155 [00:35<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 - Pérdida promedio: 2.4448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 155/155 [00:32<00:00,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 - Pérdida promedio: 2.3835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 - Pérdida promedio: 2.3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 155/155 [00:32<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 - Pérdida promedio: 2.2786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 155/155 [00:32<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 - Pérdida promedio: 2.2722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 155/155 [00:33<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 - Pérdida promedio: 2.2047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 155/155 [00:32<00:00,  4.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 - Pérdida promedio: 2.1973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 155/155 [00:32<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 - Pérdida promedio: 2.1281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 155/155 [00:32<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 - Pérdida promedio: 2.1144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 155/155 [00:32<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 - Pérdida promedio: 2.0418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 155/155 [00:32<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 - Pérdida promedio: 2.0198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 - Pérdida promedio: 1.9520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 - Pérdida promedio: 1.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 155/155 [00:31<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 - Pérdida promedio: 1.9191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 155/155 [00:31<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 - Pérdida promedio: 1.8650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 155/155 [00:32<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 - Pérdida promedio: 1.8466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 155/155 [00:32<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 - Pérdida promedio: 1.8352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 155/155 [00:32<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 - Pérdida promedio: 1.7515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 155/155 [00:32<00:00,  4.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 - Pérdida promedio: 1.7539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 155/155 [00:32<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 - Pérdida promedio: 1.7331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 155/155 [00:33<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 - Pérdida promedio: 1.7269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 155/155 [00:32<00:00,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 - Pérdida promedio: 1.6974\n",
            "Entrenamiento finalizado\n",
            "Modelo guardado como seq2seq_glove_lstm.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# Inferencia del modelo Seq2Seq\n",
        "# ================================================================================\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Cargo modelo guardado\n",
        "encoder = Encoder(embedding_inputs, hidden_dim=128, dropout=0.2)\n",
        "decoder = Decoder(vocab_size=len(word2idx_outputs), embedding_dim=300, hidden_dim=128, dropout=0.2)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "model.load_state_dict(torch.load(\"seq2seq_glove_lstm.pt\", map_location=device))\n",
        "model.eval()\n",
        "print(\"Modelo cargado correctamente\")\n",
        "\n",
        "# Crear diccionarios inversos\n",
        "idx2word_inputs = {v: k for k, v in word2idx_inputs.items()}\n",
        "idx2word_outputs = {v: k for k, v in word2idx_outputs.items()}\n",
        "\n",
        "# Función auxiliar: texto → tensor\n",
        "def sentence_to_tensor(sentence, word2idx, max_len):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'\\W+', ' ', sentence)\n",
        "    tokens = sentence.strip().split()\n",
        "    seq = [word2idx.get(w, word2idx[\"<unk>\"]) for w in tokens]\n",
        "    seq = seq[:max_len]\n",
        "    seq += [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
        "    return torch.tensor(seq).unsqueeze(0)  # shape [1, max_len]\n",
        "\n",
        "# Función de generación (inferencia paso a paso)\n",
        "def generate_reply(sentence, max_len=15):\n",
        "    with torch.no_grad():\n",
        "        src = sentence_to_tensor(sentence, word2idx_inputs, max_input_len).to(device)\n",
        "        hidden, cell = model.encoder(src)\n",
        "\n",
        "        # Primer token de entrada al decoder (<sos>)\n",
        "        input_token = torch.tensor([word2idx_outputs[\"<sos>\"]]).to(device)\n",
        "        output_sentence = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            pred_token = output.argmax(1).item()\n",
        "            pred_word = idx2word_outputs.get(pred_token, \"<unk>\")\n",
        "\n",
        "            if pred_word == \"<eos>\" or pred_word == \"<pad>\":\n",
        "                break\n",
        "\n",
        "            output_sentence.append(pred_word)\n",
        "            input_token = torch.tensor([pred_token]).to(device)\n",
        "\n",
        "    return \" \".join(output_sentence)\n",
        "\n",
        "test_questions = [\n",
        "    \"Do you read?\",\n",
        "    \"Do you have any pet?\",\n",
        "    \"Where are you from?\",\n",
        "    \"What do you like to eat?\",\n",
        "    \"Are you a student?\"\n",
        "]\n",
        "\n",
        "print(\"Chatbot automático — probando preguntas típicas:\\n\")\n",
        "\n",
        "for q in test_questions:\n",
        "    reply = generate_reply(q)\n",
        "    print(f\"You: {q}\")\n",
        "    print(f\"Bot: {reply}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0HlrVVwZETa",
        "outputId": "f56f40ea-3729-4b74-d656-4eebad918330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cargado correctamente\n",
            "Chatbot automático — probando preguntas típicas:\n",
            "\n",
            "You: Do you read?\n",
            "Bot: do not like to i am\n",
            "\n",
            "You: Do you have any pet?\n",
            "Bot: have but\n",
            "\n",
            "You: Where are you from?\n",
            "Bot: am from the how about you\n",
            "\n",
            "You: What do you like to eat?\n",
            "Bot: i not play video\n",
            "\n",
            "You: Are you a student?\n",
            "Bot: i am i you you\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo aprendió algunos patrones básicos del diálogo, como frases comunes (“I am”, “how about you”), pero aun así, sus respuestas siguen siendo algo incoherentes o gramaticalmente incorrectas, pienso que es algo normal en este tipo de modelos con un desarrollo muy simple.\n",
        "\n",
        "En general, se logra cierta fluidez, pero le cuesta mantener el contexto o generar respuestas variadas. Creo que se debe principalmente al tamaño limitado del dataset y a la falta de componentes o tecnicas más avanzadas.\n",
        "\n",
        "Para mejorarlo, se podrían:\n",
        "- Continuar con mas epochs de entrenamiento.\n",
        "- Mejorar los datos o usar modelos preentrenados.\n",
        "- Ajustar el teacher forcing para hacerlo más autónomo al generar texto.\n",
        "- Limpiar mejor los datos de entrenamiento para evitar respuestas confusas."
      ],
      "metadata": {
        "id": "CNkE9Nbgd5q2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"cells":[{"cell_type":"markdown","source":["# Procesamiento del lenguaje natural - 2025 - B4 - Desafio 3\n","**Inteligencia Artificial - CEIA - FIUBA**\n","\n","## Autor\n","\n","- **Mendoza Dante**.\n","- **SIU: e2206**.\n","\n","**Nota:** Tomé como base el código compartido por los docentes. Como la idea era pasar de Keras a PyTorch, utilicé IA para consultar errores y adaptación de código. Las que utilicé fueron Copilot y ChatGPT. Elegí un ebook encontrado en el sitio: https://www.textos.info/alejandro-dumas/el-conde-de-montecristo/ebook"],"metadata":{"id":"B_R4c51a0zGI"}},{"cell_type":"markdown","metadata":{"id":"zq6j8LsYq1Dr"},"source":["## Modelo de lenguaje con tokenización por caracteres\n","### Consigna\n","- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n","- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n","- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n","- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n","\n","### Sugerencias\n","- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n","- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n","- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros."]},{"cell_type":"markdown","source":["Conclusiones\n","\n","ALGO..."],"metadata":{"id":"wsw0OLXaQxOr"}},{"cell_type":"code","source":["################################################################################\n","# Bibliotecas y entorno CPU o GPU\n","################################################################################\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import numpy as np\n","import math\n","from tqdm import tqdm\n","import random\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","import gradio as gr\n","\n","# CPU o GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSpUUlOrsbti","executionInfo":{"status":"ok","timestamp":1759545345013,"user_tz":180,"elapsed":11946,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}},"outputId":"701f5ed2-2215-4522-a99c-a23845a8279a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["# ==============================\n","# Variables globales\n","# ==============================\n","EPOCHS = 10\n","PATIENCE = 5\n","MAX_LEN = 10 # Caracteres\n","TEMPERATURE = 0.8"],"metadata":{"id":"Bu4HmUoOHrnI","executionInfo":{"status":"ok","timestamp":1759545355081,"user_tz":180,"elapsed":10,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Descarga del texto\n","################################################################################\n","url = \"https://www.textos.info/alejandro-dumas/el-conde-de-montecristo/ebook\"\n","html = requests.get(url).content\n","soup = BeautifulSoup(html, \"lxml\")\n","paragraphs = soup.find_all(\"p\")\n","text = \" \".join(p.text for p in paragraphs)\n","\n","# limpieza básica: minúsculas y caracteres permitidos\n","text = text.lower()\n","text = re.sub(r\"[^a-záéíóúüñ0-9,.!?;:()\\s-]\", \" \", text)\n","\n","print(\"Longitud del corpus:\", len(text))\n","print(\"Ejemplo:\", text[:500])"],"metadata":{"id":"mUqy1c0VEvHi","executionInfo":{"status":"ok","timestamp":1759545361735,"user_tz":180,"elapsed":4019,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a18c015-83eb-4884-95a8-8a6be8438c8c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Longitud del corpus: 2573547\n","Ejemplo:  el 24 de febrero de 1815, el vigía de nuestra señora de la guarda dio\r\n"," la señal de que se hallaba a la vista el bergantín el faraón procedente\r\n"," de esmirna, trieste y nápoles. como suele hacerse en tales casos, salió\r\n"," inmediatamente en su busca un práctico, que pasó por delante del \r\n","castillo de if y subió a bordo del buque entre la isla de rión y el cabo\r\n"," mongión. en un instante, y también como de costum bre, se llenó de \r\n","curiosos la plataforma del castillo de san juan, por que en marsella\n"]}]},{"cell_type":"code","source":["################################################################################\n","# Tokenización\n","################################################################################\n","chars_vocab = sorted(list(set(text)))\n","vocab_size = len(chars_vocab)\n","\n","char2idx = {c:i for i,c in enumerate(chars_vocab)}\n","idx2char = {i:c for c,i in char2idx.items()}\n","\n","# tokenizo\n","tokenized_text = [char2idx[c] for c in text]"],"metadata":{"id":"OPDJhCV4QMlL","executionInfo":{"status":"ok","timestamp":1759545368853,"user_tz":180,"elapsed":123,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Preparo el dataset\n","################################################################################\n","max_context_size = 100\n","p_val = 0.1\n","num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))\n","\n","train_text = tokenized_text[:-num_val*max_context_size]\n","val_text = tokenized_text[-num_val*max_context_size:]\n","\n","train_seqs = [train_text[i:i+max_context_size] for i in range(len(train_text)-max_context_size)]\n","val_seqs = [val_text[i*max_context_size:(i+1)*max_context_size] for i in range(num_val)]\n","\n","X_train = np.array(train_seqs[:-1])\n","y_train = np.array(train_seqs[1:])\n","\n","X_val = np.array(val_seqs[:-1])\n","y_val = np.array(val_seqs[1:])"],"metadata":{"id":"FKWSmSNGQ3Jc","executionInfo":{"status":"ok","timestamp":1759545407968,"user_tz":180,"elapsed":36203,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Adapto a PyTorch\n","################################################################################\n","class CharDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.long)\n","        self.y = torch.tensor(y, dtype=torch.long)\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","batch_size = 256\n","train_loader = DataLoader(CharDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(CharDataset(X_val, y_val), batch_size=batch_size, shuffle=False)"],"metadata":{"id":"Ji9mZ0AAQ95X","executionInfo":{"status":"ok","timestamp":1759545433211,"user_tz":180,"elapsed":1961,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Modelo RNN con Embeddings\n","################################################################################\n","class CharRNN(nn.Module):\n","    def __init__(self, model_type, vocab_size, emb_size=64, hidden_size=200, n_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_size)\n","        self.model_type = model_type.lower()\n","        if self.model_type == \"simplernn\":\n","            self.rnn = nn.RNN(emb_size, hidden_size, num_layers=n_layers, batch_first=True, nonlinearity='tanh', dropout=dropout if n_layers>1 else 0)\n","        elif self.model_type == \"lstm\":\n","            self.rnn = nn.LSTM(emb_size, hidden_size, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers>1 else 0)\n","        elif self.model_type == \"gru\":\n","            self.rnn = nn.GRU(emb_size, hidden_size, num_layers=n_layers, batch_first=True, dropout=dropout if n_layers>1 else 0)\n","        else:\n","            raise ValueError(\"model_type debe ser simplernn|lstm|gru\")\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x, hidden=None):\n","        emb = self.emb(x)\n","        out, hidden = self.rnn(emb, hidden)\n","        logits = self.fc(out)\n","        return logits, hidden\n","\n","    def init_hidden(self, batch_size):\n","        num_layers = self.rnn.num_layers\n","        hidden_size = self.rnn.hidden_size\n","        if self.model_type == \"lstm\":\n","            h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n","            c0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n","            return (h0, c0)\n","        else:\n","            h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n","            return h0"],"metadata":{"id":"6aRVNJ5cRFJh","executionInfo":{"status":"ok","timestamp":1759545448046,"user_tz":180,"elapsed":18,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Loop de entrenamiento con perplejidad\n","################################################################################\n","def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    total_tokens = 0\n","    with torch.no_grad():\n","        for x, y in dataloader:\n","            x, y = x.to(device), y.to(device)\n","            logits, _ = model(x)\n","            B,T,V = logits.size()\n","            loss = criterion(logits.view(B*T,V), y.view(B*T))\n","            total_loss += loss.item()*(B*T)\n","            total_tokens += B*T\n","    avg_loss = total_loss / total_tokens\n","    return avg_loss, math.exp(avg_loss)\n","\n","def train(model, train_loader, val_loader, epochs=EPOCHS, lr=1e-3, clip=5.0, patience=PATIENCE):\n","    model.to(device)\n","    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    best_ppl = float('inf')\n","    patience_counter = 0\n","    best_state = None\n","\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        total_loss = 0\n","        total_tokens = 0\n","        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n","            x, y = x.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            logits, _ = model(x)\n","            B,T,V = logits.size()\n","            loss = criterion(logits.view(B*T,V), y.view(B*T))\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","            total_loss += loss.item()*(B*T)\n","            total_tokens += B*T\n","        train_loss = total_loss / total_tokens\n","\n","        val_loss, val_ppl = evaluate(model, val_loader, criterion)\n","        print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_ppl {val_ppl:.4f}\")\n","\n","        if val_ppl < best_ppl:\n","            best_ppl = val_ppl\n","            best_state = model.state_dict()\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping por perplejidad\")\n","                break\n","    model.load_state_dict(best_state)\n","    return model"],"metadata":{"id":"w1sl7VnVRL4U","executionInfo":{"status":"ok","timestamp":1759545453502,"user_tz":180,"elapsed":18,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Funciones de generación\n","################################################################################\n","def greedy_generate(model, seed, stoi, itos, max_len=MAX_LEN):\n","    model.eval()\n","    idxs = [stoi[c] for c in seed if c in stoi]\n","    input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n","    hidden = None\n","    generated = idxs.copy()\n","    with torch.no_grad():\n","        for _ in range(max_len):\n","            logits, hidden = model(input_seq, hidden)\n","            probs = F.softmax(logits[:, -1, :], dim=-1)\n","            topi = torch.argmax(probs, dim=-1).item()\n","            generated.append(topi)\n","            input_seq = torch.tensor([[topi]], device=device)\n","    return \"\".join([itos[i] for i in generated])\n","\n","def stochastic_generate(model, seed, stoi, itos, max_len=MAX_LEN, temperature=TEMPERATURE):\n","    model.eval()\n","    idxs = [stoi[c] for c in seed if c in stoi]\n","    input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n","    hidden = None\n","    generated = idxs.copy()\n","    with torch.no_grad():\n","        for _ in range(max_len):\n","            logits, hidden = model(input_seq, hidden)\n","            probs = F.softmax(logits[:, -1, :].squeeze(0)/temperature, dim=-1)\n","            next_idx = torch.multinomial(probs, num_samples=1).item()\n","            generated.append(next_idx)\n","            input_seq = torch.tensor([[next_idx]], device=device)\n","    return \"\".join([itos[i] for i in generated])"],"metadata":{"id":"aHMyjaP-RUEs","executionInfo":{"status":"ok","timestamp":1759545459650,"user_tz":180,"elapsed":7,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Interfaz Gradio\n","################################################################################\n","def model_response(ingresar_texto):\n","    output_text = greedy_generate(model, ingresar_texto, char2idx, idx2char, max_len=MAX_LEN)\n","    return output_text\n","\n","iface = gr.Interface(fn=model_response, inputs=\"text\", outputs=\"text\")"],"metadata":{"id":"iC9XAXnevHgH","executionInfo":{"status":"ok","timestamp":1759545467310,"user_tz":180,"elapsed":187,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Entreno el modelo\n","################################################################################\n","model_type = \"lstm\"  # simplernn | lstm | gru\n","model = CharRNN(model_type, vocab_size=vocab_size, emb_size=64, hidden_size=200, n_layers=2, dropout=0.1)\n","model = train(model, train_loader, val_loader, epochs=EPOCHS, lr=1e-3, clip=5.0, patience=PATIENCE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvMD-veDSXKi","executionInfo":{"status":"ok","timestamp":1759547683765,"user_tz":180,"elapsed":2211983,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}},"outputId":"ff2e969d-85c4-4d47-f327-8cc5c10667bd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 9048/9048 [06:06<00:00, 24.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | train_loss 1.2482 | val_loss 7.9340 | val_ppl 2790.4362\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 9048/9048 [06:07<00:00, 24.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | train_loss 1.0852 | val_loss 8.3614 | val_ppl 4278.7209\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 9048/9048 [06:07<00:00, 24.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | train_loss 1.0575 | val_loss 8.5659 | val_ppl 5249.3160\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 9048/9048 [06:07<00:00, 24.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | train_loss 1.0448 | val_loss 8.6300 | val_ppl 5596.9380\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 9048/9048 [06:07<00:00, 24.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | train_loss 1.0374 | val_loss 8.8186 | val_ppl 6758.8649\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 9048/9048 [06:07<00:00, 24.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 | train_loss 1.0326 | val_loss 8.9481 | val_ppl 7693.2326\n","Early stopping por perplejidad\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["################################################################################\n","# Guardado del modelo\n","################################################################################\n","torch.save(model, \"char_rnn_full.pth\")"],"metadata":{"id":"a7MmDGVKY0tZ","executionInfo":{"status":"ok","timestamp":1759547731464,"user_tz":180,"elapsed":38,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Carga del modelo\n","################################################################################\n","model = torch.load(\"char_rnn_full.pth\")\n","model.to(device)\n","model.eval()"],"metadata":{"id":"7Xb9C2Xr2U0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Lanzo Gradio\n","################################################################################\n","iface.launch(debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"o-tFODnB1eZm","executionInfo":{"status":"ok","timestamp":1759547808896,"user_tz":180,"elapsed":63663,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}},"outputId":"c4215ded-e5b0-4bf0-8598-c4310e58c205"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://7d37daa90df79c8368.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://7d37daa90df79c8368.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://7d37daa90df79c8368.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["################################################################################\n","# Beam Search (determinístico y estocástico)\n","################################################################################\n","from scipy.special import softmax\n","\n","def beam_search(model, seed, stoi, itos, max_len=MAX_LEN, beam_width=3, temperature=TEMPERATURE, stochastic=False):\n","    model.eval()\n","    idxs = [stoi[c] for c in seed if c in stoi]\n","    sequences = [(idxs, 0.0, None)]\n","\n","    with torch.no_grad():\n","        # inicializo hidden con todo el contexto\n","        input_seq = torch.tensor(idxs, device=device).unsqueeze(0)\n","        _, hidden = model(input_seq, None)\n","\n","        for _ in range(max_len):\n","            all_candidates = []\n","            for seq, log_prob, hid in sequences:\n","                last_token = torch.tensor([[seq[-1]]], device=device)\n","                logits, new_hid = model(last_token, hidden if hid is None else hid)\n","                probs = F.softmax(logits[:, -1, :].squeeze(0), dim=-1).cpu().numpy()\n","\n","                if stochastic:\n","                    # muestreo estocástico con temperatura\n","                    probs_temp = softmax(np.log(probs+1e-20)/temperature)\n","                    top_idxs = np.random.choice(len(probs_temp), beam_width, p=probs_temp)\n","                else:\n","                    # determinístico: top k\n","                    top_idxs = probs.argsort()[-beam_width:][::-1]\n","\n","                for idx in top_idxs:\n","                    candidate = (seq + [idx], log_prob + math.log(probs[idx]+1e-20), new_hid)\n","                    all_candidates.append(candidate)\n","\n","            # me quedo con los beam_width mejores\n","            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n","\n","        # devuelvo la secuencia con mayor log_prob\n","        best_seq = sequences[0][0]\n","        return \"\".join([itos[i] for i in best_seq])"],"metadata":{"id":"16yePuHAcZpZ","executionInfo":{"status":"ok","timestamp":1759547817780,"user_tz":180,"elapsed":270,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","# Utilizo Beam Search (determinístico y estocástico)\n","################################################################################\n","seed_text = \"el cond\"\n","print(\"Greedy:\", greedy_generate(model, seed_text, char2idx, idx2char, max_len=MAX_LEN))\n","print(\"Beam search det:\", beam_search(model, seed_text, char2idx, idx2char, max_len=MAX_LEN, beam_width=5, stochastic=False))\n","print(f\"Beam search sto (T={TEMPERATURE}):\", beam_search(model, seed_text, char2idx, idx2char, max_len=MAX_LEN, beam_width=5, stochastic=True, temperature=TEMPERATURE))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnRTuNt7cbNV","executionInfo":{"status":"ok","timestamp":1759547926466,"user_tz":180,"elapsed":93,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}},"outputId":"99f55c14-6be9-4b31-b841-c8ae9a5ee408"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Greedy: el conde de monte\n","Beam search det: el conde de morce\n","Beam search sto (T=0.8): el condo de monte\n"]}]},{"cell_type":"code","source":["# %% Gradio con opciones de generación\n","def model_response_advanced(ingresar_texto, method, beam_width=5, temperature=TEMPERATURE, max_len=MAX_LEN):\n","    if method == \"Greedy\":\n","        return greedy_generate(model, ingresar_texto, char2idx, idx2char, max_len=max_len)\n","    elif method == \"Beam Determinístico\":\n","        return beam_search(model, ingresar_texto, char2idx, idx2char, max_len=max_len, beam_width=beam_width, stochastic=False)\n","    elif method == \"Beam Estocástico\":\n","        return beam_search(model, ingresar_texto, char2idx, idx2char, max_len=max_len, beam_width=beam_width, stochastic=True, temperature=temperature)\n","    else:\n","        return \"Método no reconocido\"\n","\n","iface_advanced = gr.Interface(\n","    fn=model_response_advanced,\n","    inputs=[\n","        gr.Textbox(label=\"Texto inicial\"),\n","        gr.Dropdown([\"Greedy\", \"Beam Determinístico\", \"Beam Estocástico\"], label=\"Método de generación\"),\n","        gr.Slider(1, 10, value=5, step=1, label=\"Beam Width\"),\n","        gr.Slider(0.1, 2.0, value=1.0, step=0.1, label=\"Temperatura (solo Beam Estocástico)\"),\n","        gr.Slider(10, 200, value=50, step=10, label=\"Longitud máxima de generación\")\n","    ],\n","    outputs=\"text\",\n","    title=\"Generación de texto con RNN\",\n","    description=\"Selecciona el método de generación y ajusta los parámetros para probar el modelo de lenguaje.\"\n",")\n","\n","iface_advanced.launch(debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"kGUorbNndYfl","outputId":"d10dd200-62a9-4d1f-c3cc-b7034d66b852","executionInfo":{"status":"ok","timestamp":1759542516630,"user_tz":180,"elapsed":50975,"user":{"displayName":"Héctor Dante Mendoza","userId":"04431454177187757982"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://e67083f63d6ddf0ea9.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://e67083f63d6ddf0ea9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://e67083f63d6ddf0ea9.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":24}]}],"metadata":{"colab":{"provenance":[{"file_id":"182k2CKnwqV2zfyEnbgu0aKNTdMvQA37y","timestamp":1759086018224}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}